{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GmOe6101F_8",
        "colab_type": "text"
      },
      "source": [
        "Took help from the follwing repository: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb. It helped in building the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBe3Mdfc3nBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHgq7WNx45_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rows = []\n",
        "labels = []\n",
        "titles = []\n",
        "posts = []\n",
        "\n",
        "with open('train.csv') as train_file:\n",
        "  reader = csv.reader(train_file)\n",
        "  for row in reader:\n",
        "    rows.append(row)\n",
        "  for i in range(1, len(rows)):\n",
        "    labels.append(rows[i][2])\n",
        "    posts.append(rows[i][1])\n",
        "    titles.append(rows[i][0])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO1HqFMK-iSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9931cfbb-655b-40a2-e879-6f9107b8893f"
      },
      "source": [
        "print(titles[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "netflix the family has been an amazing watch do you think the rssvhp has been what family at c street has been to the bjp and their annual meet up is what the national prayer breakfast held in washington is all about\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75nyyapTK5KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = open('titles.txt', 'w')\n",
        "p = open('posts.txt', 'w')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6PF1VEnLjHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = open('labels.txt', 'w')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQdS_fCg8KV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37cf5278-7e1b-42d8-de67-21bfa444f955"
      },
      "source": [
        "len(titles)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmA9uVlmg-j9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68ec5a82-c8e8-43bd-d595-d9120e3e0411"
      },
      "source": [
        "len(posts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXZ69lTFhBJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eee2d1fe-9cb4-4f59-8054-836df6eaed72"
      },
      "source": [
        "len(labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e75_WC83KCff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(titles)):\n",
        "  if i == len(titles) - 1:\n",
        "    t.write(titles[i])\n",
        "  else:\n",
        "    t.write(titles[i] + '\\n')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IexlbXsmepe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46bbb63-f08f-457f-9b1a-58af073348e9"
      },
      "source": [
        "min(labels)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPAQSzjoK6V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(posts)):\n",
        "  if i == len(posts) - 1:\n",
        "    p.write(posts[i])\n",
        "  else:\n",
        "    p.write(posts[i] + '\\n')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a33M0bkQLnIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(labels)):\n",
        "  if i == len(labels) - 1:\n",
        "    l.write(labels[i])\n",
        "  else:\n",
        "    l.write(labels[i] + '\\n')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGStG3eQLQh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('titles.txt') as t:\n",
        "  title = t.read()\n",
        "with open('posts.txt') as p:\n",
        "  post = p.read()\n",
        "with open('labels.txt') as l:\n",
        "  label = l.read()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hAN6priMCGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get rid of punctuation\n",
        "title = title.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in title if c not in punctuation])\n",
        "\n",
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words_t = all_text.split()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKIkvsuKMCRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get rid of punctuation\n",
        "post = post.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in post if c not in punctuation])\n",
        "\n",
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words_p = all_text.split()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlvsOTtIMCdH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f18c21e4-6158-404f-b6a0-67a431afd415"
      },
      "source": [
        "words_t[:30]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['netflix',\n",
              " 'the',\n",
              " 'family',\n",
              " 'has',\n",
              " 'been',\n",
              " 'an',\n",
              " 'amazing',\n",
              " 'watch',\n",
              " 'do',\n",
              " 'you',\n",
              " 'think',\n",
              " 'the',\n",
              " 'rssvhp',\n",
              " 'has',\n",
              " 'been',\n",
              " 'what',\n",
              " 'family',\n",
              " 'at',\n",
              " 'c',\n",
              " 'street',\n",
              " 'has',\n",
              " 'been',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bjp',\n",
              " 'and',\n",
              " 'their',\n",
              " 'annual',\n",
              " 'meet',\n",
              " 'up']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vAdDQ9CMCmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words_p)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "title_ints = []\n",
        "for titl in reviews_split:\n",
        "    title_ints.append([vocab_to_int[word] for word in titl.split()])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K0Nu0UCMCuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e61766ac-c7ce-43bf-e47d-f2ac49afd1ad"
      },
      "source": [
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "print('Tokenized review: \\n', title_ints[:1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  91284\n",
            "\n",
            "Tokenized review: \n",
            " [[31457, 117, 1028, 1, 159, 8, 36, 5, 17213, 403, 6, 22880, 3188, 519, 55, 14, 881, 4481, 2, 48, 6, 1, 105, 1952, 42, 7, 11237, 7421]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKzskNSMC1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_split = label.split('\\n')\n",
        "labels_split = labels_split[:36548]\n",
        "encoded_labels = np.array([int(label_value) for label_value in labels_split])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_8EF2OYSmWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2535235-f1f1-44e8-ac35-b470b54debd7"
      },
      "source": [
        "len(encoded_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWs9uwV6MC7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(encoded_labels)):\n",
        "  encoded_labels[i] = int(encoded_labels[i])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAfDjKRaOYwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04c49b0e-614e-402a-e3fc-48c61e3184aa"
      },
      "source": [
        "type(encoded_labels[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej9KaENkOY8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eb882ab1-b6b7-4d05-c6d5-06e0b98dff6e"
      },
      "source": [
        "# Outliers\n",
        "title_lens = Counter([len(x) for x in title_ints])\n",
        "print(\"Zero-length titles: {}\".format(title_lens[0]))\n",
        "print(\"Maximum title length: {}\".format(max(title_lens)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 505\n",
            "Maximum review length: 4379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDrAQbLCOZEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "05402c41-aba4-4ae3-84be-a40d7a957554"
      },
      "source": [
        "print('Number of titles before removing outliers: ', len(title_ints))\n",
        "\n",
        "non_zero_idx = [ii for ii, review in enumerate(title_ints) if len(review) != 0]\n",
        "\n",
        "reviews_ints = [title_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of titles after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  36547\n",
            "Number of reviews after removing outliers:  36042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfCJtLvzOZKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    # Pads or truncates the titles to a fixed length given by seq_length\n",
        "    \n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "    for i, row in enumerate(reviews_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CH2ecWNWEqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "8b53d8e9-46f4-41ca-e7f0-f18a3d0bfacb"
      },
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "print(features[:30,:10])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [ 423  368    5  289  806  572  216  320  909   17]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   3   50  109    5  147  558   26    5 1901  470]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV4-YFMiWE3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the data\n",
        "train_x, train_y = features, encoded_labels\n",
        "train_data = torch.utils.data.TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# Define the train_loader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFQtFjiHWE-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "33bc23bd-a8cb-4f52-f08f-ed055a51e0a2"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ..., 16094,     7,  1548],\n",
            "        [    0,     0,     0,  ...,     0,     0, 19359],\n",
            "        [    0,     0,     0,  ...,   174,    41,  2849],\n",
            "        ...,\n",
            "        [    5,   117,   487,  ...,     2,    35,   367],\n",
            "        [  410,    42,     3,  ...,    99,  1172,  1416],\n",
            "        [    0,     0,     0,  ...,     3,   116, 38441]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([14,  7,  0,  0,  0, 10,  0,  0,  0,  0, 10,  7,  0, 10,  7,  0, 14,  0,\n",
            "        10, 10,  0,  0,  0,  0,  0,  1,  0,  3,  0,  0,  7,  0,  0, 10,  0,  0,\n",
            "         0,  0,  7,  7,  4,  0,  0,  9,  7,  9,  9, 12,  0,  0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLYWAXWVZt72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c01ee899-d36b-454f-cf68-b131fbc890c5"
      },
      "source": [
        "# Checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfJKAlFMZuKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        #self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        #print(x.shape)\n",
        "        embeds = self.embedding(x)\n",
        "        #print(x.shape)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        #print(lstm_out.shape)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        #print(lstm_out.shape)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        #print(out.shape)\n",
        "        out = self.fc(out)\n",
        "        #print(out.shape)\n",
        "        # softmax function\n",
        "        #out = self.softmax(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        out = out.view(batch_size, -1)\n",
        "        #print(out.shape)\n",
        "        out = out[:, -1] # get last batch of labels\n",
        "        #print(out.shape)\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6mbAJhLZuUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "89d9519f-6ac0-45e0-d6a3-b3086d2fc3e1"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 15\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(91285, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=15, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPICP_IsZua7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IScDCUpZuhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "3e14b1be-131d-4cac-ffeb-98749498f3c5"
      },
      "source": [
        "# Training\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5 # gradient clipping to prevent exploding gradient problem\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        #print(torch.argmax(labels))\n",
        "        #print(labels)\n",
        "        #print(inputs)\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Create new varaiables to prevent back propogating through the entire history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # Zero the accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "        output = output.unsqueeze(dim=0)\n",
        "        print(output.shape)\n",
        "\n",
        "        labels = torch.argmax(labels)\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "    print('Epoch: {}'.format(e+1),\n",
        "          'Training Loss: {}'.format(loss.item()))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(41)\n",
            "THIS IS THE BATCH_SIZE: 50\n",
            "torch.Size([50, 200])\n",
            "torch.Size([50, 200])\n",
            "torch.Size([50, 200, 256])\n",
            "torch.Size([10000, 256])\n",
            "torch.Size([10000, 256])\n",
            "torch.Size([10000, 15])\n",
            "torch.Size([50, 3000])\n",
            "torch.Size([50])\n",
            "torch.Size([1, 50])\n",
            "This is output: tensor([[0.0707, 0.0718, 0.0704, 0.0690, 0.0735, 0.0668, 0.0704, 0.0702, 0.0733,\n",
            "         0.0705, 0.0734, 0.0683, 0.0688, 0.0735, 0.0679, 0.0705, 0.0733, 0.0689,\n",
            "         0.0711, 0.0676, 0.0732, 0.0683, 0.0707, 0.0714, 0.0696, 0.0686, 0.0697,\n",
            "         0.0691, 0.0686, 0.0713, 0.0697, 0.0707, 0.0747, 0.0694, 0.0695, 0.0703,\n",
            "         0.0715, 0.0741, 0.0680, 0.0688, 0.0707, 0.0706, 0.0685, 0.0663, 0.0696,\n",
            "         0.0718, 0.0708, 0.0703, 0.0703, 0.0678]],\n",
            "       grad_fn=<UnsqueezeBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-e7674f353fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is output: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is new output: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Float (got Long)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn4K-dFVZuop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}